.. _testing:

Testing
=======

.. include:: /_static/feedback.rst

Even with models that are formulated and coded by hand, testing and validation of the results is an important step. In practice, there is often an iterative cycle of:

- Develop/formalize problem description
- Formulate mathematical model
- Implement and solve model
- Review and validate the solution
	
Invalid results could have many causes, such as:

- Inaccuracies in the initial problem description
- Mistakes or over-simplifications in formulating the mathematical model for the problem
- Mistakes or oversights in implementing the model
- Inaccuracies in the data.

Corrections/revisions might need to be made at any of the model development stages in response to any of the above root causes. For models and code generated by the Gurobi Model Builder, errors could occur in any of these places as well.

To fully validate a model, it would be necessary to review the formulation, code and solution for accuracy, and make updates and revisions as necessary. For users just getting started with mathematical optimization it might be challenging to accurately review and evaluate the correctness of the model/code generated by the Gurobi Model Builder.

One good way to get started with this process is to conduct a sanity check of the solution provided.
- Ensure that the optimal solution value is plausible.
- Review the values of the decision variables and ensure that they make sense. It could be helpful to visualize the solution to look for easily detectible signs of infeasibility or suboptimality.
- Try to verify that the solution is feasible, satisfying all of the constraints in the model. Model Builder/ChatGPT could be asked to generate code to check that a proposed solution is feasible.  (Or you could try using Gurobi's new solution checker tool)

Some specific edge cases you may want to test when evaluating model validity could be:

- Test a solution of all '0' values for the decision variables.  Should this be feasible or infeasible?  Does the objective make sense?
- Test a solution where all decision variables are set to their min or max bound.  Does the objective function trend in the expected direction?  If the model is infeasible, does that make sense?
- Test a known feasible point.  Do the objective and other constraint values match what you currently observe for your application?  

If any problems are detected at this stage, users could review the prompt and resulting model/code for signs of trouble and try revising the prompt to be more clear or forceful when describing the constraints or other aspects of the problem that may have led to the failure.  Or, if you are more comfortable with the mathematical notation consider requests to modify a specific constraint in the formulation.  
